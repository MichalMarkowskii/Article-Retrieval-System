{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import uuid\n",
    "import os\n",
    "from openai import OpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T20:35:03.318681200Z",
     "start_time": "2024-04-07T20:35:01.865538300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=512):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def process_and_add_to_chromadb(input_file='data/medium.csv'):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        title = row['Title']\n",
    "        text_chunks = chunk_text(row['Text'])\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            documents.append(chunk)\n",
    "            metadatas.append({\"title\": title})\n",
    "            ids.append(str(uuid.uuid4()))\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=\"medium_db\")\n",
    "\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(name=\"medium_articles\", embedding_function=sentence_transformer_ef)\n",
    "\n",
    "    collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "process_and_add_to_chromadb()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T17:45:05.399441700Z",
     "start_time": "2024-04-07T17:43:40.136828400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def query_db(query_text, n_results=5):\n",
    "    collection_name = \"medium_articles\"\n",
    "\n",
    "    # Initialize ChromaDB client and access the collection\n",
    "    chroma_client = chromadb.PersistentClient(path=\"medium_db\")\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    # Perform the query\n",
    "    results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results,\n",
    "        include=['documents', 'metadatas']\n",
    "    )\n",
    "\n",
    "    # Prepare the documents and titles for return\n",
    "    documents = [doc for doc in results['documents'][0]]\n",
    "    titles = [meta['title'] for meta in results['metadatas'][0]]\n",
    "\n",
    "    return documents, titles\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T20:35:07.402710700Z",
     "start_time": "2024-04-07T20:35:07.380704200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "def generate_answer_with_openai(question, n_results=5):\n",
    "    documents, titles = query_db(question, n_results)\n",
    "\n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # Construct a detailed prompt from the question and the retrieved documents\n",
    "    articles = \"\"\n",
    "    prompt = f\"Question: {question}\\n\\n\"\n",
    "    for title, doc in zip(titles, documents):\n",
    "        prompt += f\"Article Title: {title}\\n{doc}\\n\\n\"\n",
    "        if articles == \"\":\n",
    "            articles += f\"Sources: {title}\"\n",
    "        else:\n",
    "            articles += f\", {title}\"\n",
    "    prompt += \"Based on the above articles, please answer the question.\"\n",
    "\n",
    "    # Check for max token limit and truncate if necessary\n",
    "    max_tokens = 4096\n",
    "    if len(prompt) > max_tokens:\n",
    "        prompt = prompt[:max_tokens]\n",
    "\n",
    "    # Generate an answer using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Adjust the model as necessary\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant who provides answers based on the provided articles.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract the generated answer\n",
    "    answer = response.choices[0].message.content + f\"\\n\\n{articles}\"\n",
    "\n",
    "    return answer.strip()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T20:42:09.782992600Z",
     "start_time": "2024-04-07T20:42:09.777999800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['So, What is Natural Language Processing (NLP)? NLP is an interdisciplinary field concerned with the interactions between computers and natural human languages (e.g. English) — speech or text. NLP-powered software helps us in our daily lives in various ways, for example: Personal assistants : Siri, Cortana, and Google Assistant. : Siri, Cortana, and Google Assistant. Auto-complete : In search engines (e.g. Google). : In search engines (e.g. Google). Spell checking : Almost everywhere, in your browser, your IDE (e.g. Visual Studio), desktop apps (e.g. Microsoft Word). : Almost everywhere, in your browser, your IDE (e.g. Visual Studio), desktop apps (e.g. Microsoft Word). Machine Translation: Google Translate. Okay, now we get it, NLP plays a significant role in our daily computer interactions; let’s take a look at some example business-related use-cases for NLP: Fast-food chains receive a vast amount of orders and complaints daily; manually handling this will be tiresome and repetitive, also inefficient in terms of time, labour and cost. Thanks to recent advancements in conversational AI , they can build virtual assistants that automate such processes and reduce human intervention. , they can build virtual assistants that automate such processes and reduce human intervention. Brands launch new products and market them on social media platforms; they can measure campaigns’ success rates using metrics such as reach and number of interactions. Still, they can’t understand the consumers’ public sentiment automatically. This task can be automated using sentiment analysis, a text classification task where machine learning models are trained to quantify affective states and subjective information. NLP is mainly divided into two fields: Linguistics and Computer Science. The Linguistics side focuses on understanding the structure of language, including the following sub-fields [Bender, 2013]: Phonetics: The study of the sounds of human language. Phonology: The study of the sound systems in human languages. Morphology: The study of the formation and internal structure of words. Syntax: The study of the formation and internal structure of sentences. Semantics: The study of the meaning of sentences. Pragmatics: The study of the way sentences with their semantic meanings are used for particular communicative goals. The Computer Science side is concerned with translating linguistic knowledge and domain expertise into computer programs with the help of sub-fields such as Artificial Intelligence.', 'NLP is short for Natural Language Processing. As you probably know, computers are not as great at understanding words as they are numbers. This is all changing though as advances in NLP are happening everyday. The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP. The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language. As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things. Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc. From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand. For this example, we’re examining a dataset of Amazon products/reviews which can be found and downloaded for free on data.world. I’ll be using Python in Jupyter notebook. Here are the imports used: (You may need to run nltk.download() in a cell if you’ve never previously used it.) Read in csv file, create DataFrame & check shape. We are starting out with 10,000 rows and 17 columns. Each row is a different product on Amazon. I conducted some basic data cleaning that I won’t go into detail about now, but you can read my post about EDA here if you want some tips. In order to make the dataset more manageable for this example, I first dropped columns with too many nulls and then dropped any remaining rows with null values. I changed the number_of_reviews column type from object to integer and then created a new DataFrame using only the rows with no more than 1 review. My new shape is 3,705 rows and 10 columns and I renamed it reviews_df . NOTE: If we were actually going to use this dataset for analysis or modeling or anything besides a text preprocessing demo, I would not recommend eliminating such a large percent of the rows. The following workflow is what I was taught to use and like using, but the steps are just general suggestions to get you started. Usually I have to modify and/or expand depending on the text format. Remove HTML Tokenization + Remove punctuation Remove stop words Lemmatization or Stemming While cleaning this data I ran into a problem I had not encountered before, and learned a cool new trick from geeksforgeeks.org to split a string from one column into multiple columns either on spaces or specified characters.', 'The aim of this article is to outline our process for using NLTK and Natural Language Processing methods to clean and preprocess text data and turn song lyrics into a matrix of numerical values, so we can train a Machine Learning Algorithm that can classify each song’s genre based on its lyrics. What is Natural Language Processing (NLP for short)? NLP refers to analytics tasks that deal with natural human language, in the form of text or speech. These tasks usually involve some sort of machine learning, whether for text classification or for feature generation, but NLP isn’t just machine learning. Tasks such as text preprocessing and cleaning also fall under the NLP umbrella. The most common python library used for NLP tasks is the Natural Language Tool Kit, or NLTK. NLTK is a sort of “one-stop shop” for all things NLP. Unlike most other Python Libraries and ML models, NLTK and NLP are unique in the sense that in addition to statistics and math, they also rely heavily on the field of Linguistics. Many of the concepts and methods for working with text data described throughout the rest of this article are grounded in linguistics rules. Obtaining Data: Where did we get our data? We found a CSV on Kaggle with 300,000 song lyrics ranging from 11 different genres and 6–7 different languages. The dataset had information on the Song Title, Artist, Year, Album, Genre, and a column with the full song Lyrics. Cleaning and Pre-Processing Text Data Now that we have our data, the fun part begins. First, we need to preprocess and clean our text data. As you might have already suspected, preprocessing text data is a bit more challenging than working with more traditional data types because there’s no clear-cut answer for exactly what sort of preprocessing and cleaning we need to do. When working with traditional datasets, our goals are generally pretty clear for this stage — normalize and clean our numerical data, convert categorical data to a numeric format, check for and deal with multicollinearity, etc. The steps we take are largely dependent on what the data already looks like when we get a hold of it.', 'Photo by Thomas Kelley on Unsplash The Natural Language Engineering journal is now in its 25th year. The editorial preface to the first issue back in 1995 emphasized that the focus of the journal was to be on the practical application of natural language processing (NLP) technologies: the time was ripe for a serious publication that helped encourage research ideas to find their way into real products. The commercialization of NLP technologies had already started by that point, but things have advanced tremendously over the last quarter-century. So, to celebrate the journal’s anniversary, we look at how commercial NLP products have developed over the last 25 years. A version of this article also appears in the May 2019 issue of Natural Language Engineering. Some Context For many researchers, work in Natural Language Processing has a dual appeal. On the one hand, the computational modelling of language understanding or language production has often been seen as means of exploring theoretical questions in both linguistics and psycholinguistics, the general argument being that, if you can build a computational model of some phenomenon, then you have likely moved some way towards an understanding of that phenomenon. On the other hand, the scope for practical applications of natural language processing technologies has always been enticing: the idea that we could build truly useful computational artifacts that work with human language goes right back to the origins of the field in the early machine translation experiments of the 1950s. However, it was in the early 1990s that commercial applications of NLP really started to flourish, pushed forward in particular by targeted research in both the US, much of it funded by DARPA via programs like the Message Understanding Conferences (MUC), and in Europe, via a number of large-scale forward-looking EU-funded research programs. It was against this backdrop of activity that the Journal of Natural Language Engineering came into being, with an explicit goal of focusing primarily on practical rather than theoretical contributions. With the journal now in its 25th year, we have a nice excuse to look at the history of NLP commercialization, and to reflect on how far we have come in those 25 years. Starting Points As well as being the year in which the journal began, 1995 was notable for a considerable number of other computing-related events. It was the year in which Windows 95 and Netscape became available; Java 1.0 appeared, and JavaScript was developed; DVDs were introduced, and Sony released the PlayStation in North America. It was also the year in which NSFNet was decommissioned, removing the last restrictions on the commercialization of the Internet. Yahoo.com, eBay.com and Amazon.com all launched in 1995. And IBM unveiled Deep Blue, the computing system that went on to beat world chess champion Garry Kasparov. Ah, those were heady days! Conveniently for our present purposes, 1995 was also the year in which Ken Church and Lisa Rau published an article on the commercialization of NLP. It’s worth reviewing how the field appeared at that time. Church and Rau looked at developments in four areas: NLP generally,', 'Word Embeddings for NLP In this article, we will understand how to process text for usage in machine learning algorithms. What are embeddings and why are they used for text processing? word2vec and GloVe word embeddings Natural Language Processing(NLP) refers to computer systems designed to understand human language. Human language, like English or Hindi consists of words and sentences, and NLP attempts to extract information from these sentences. A few of the tasks that NLP is used for Text summarization: extractive or abstractive text summarization Sentiment Analysis Translating from one language to another: neural machine translation Chatbots Machine learning and deep learning algorithms only take numeric input so how do we convert text to numbers? Bag of words(BOW) Bag of words is a simple and popular technique for feature extraction from text. Bag of word model processes the text to find how many times each word appeared in the sentence. This is also called as vectorization. Steps for creating BOW Tokenize the text into sentences Tokenize sentences into words Remove punctuation or stop words Convert the words to lower text Create the frequency distribution of words In the code below, we use CountVectorizer, it tokenizes a collection of text documents, builds a vocabulary of known words, and encodes new documents using that vocabulary.'], ['Introduction to Natural Language Processing (NLP)', 'NLP for Beginners: Cleaning & Preprocessing Text Data', 'How We Used NLTK and NLP to Predict a Song’s Genre From Its Lyrics', 'NLP commercialization in the last 25 years', 'Word Embeddings for NLP'])\n"
     ]
    }
   ],
   "source": [
    "print(query_db(\"What is NLP?\", 5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T20:42:10.203672600Z",
     "start_time": "2024-04-07T20:42:10.183672Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Word2Vec is a technique used to learn word embeddings by using a two-layer neural network. It takes a text corpus as input and generates a set of vectors as output. These word embeddings make natural language computer-readable, allowing mathematical operations to be performed on words to detect similarities. With Word2Vec, similar words tend to cluster together in a vector space. There are two main training algorithms for Word2Vec: continuous bag of words (CBOW) and skip-gram. Skip-gram is often preferred because it can capture multiple semantics for a single word, resulting in more accurate representations. The Gensim Python library is commonly used for implementing Word2Vec models for word embeddings.\n",
      "\n",
      "Sources: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model, Sentiment Analysis — A how-to guide with movie reviews, Word Embeddings for NLP, Wine Embeddings and a Wine Recommender, Spam Filtering System With Deep Learning\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Word2Vec?\"\n",
    "answer = generate_answer_with_openai(question, 5)\n",
    "print(\"Answer:\", answer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T20:46:28.799013600Z",
     "start_time": "2024-04-07T20:46:26.052719Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
